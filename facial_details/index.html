<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo">
    <meta name="author" content="Anpei Chen,
                                Zhang Chen,
                                Guli Zhang,
                                Ziheng Zhang,
								Kenny Mitchell,
                                Jingyi Yu">

    <title>Photo-Realistic Facial Details Synthesis from Single Image</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Photo-Realistic Facial Details Synthesis from Single Image</h2>
<!--     <h3>NeurIPS 2020 (Oral)</h3>
           <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p class="authors">
        <a href="https://apchenstu.github.io/"> Anpei Chen</a>,
        <a href="https://scholar.google.com/citations?user=4MIbSrAAAAAJ&hl=en"> Zhang Chen</a>,
        <a > Guli Zhang</a>,
        <a href="https://www.aiyoggle.me/"> Ziheng Zhang</a></br>
        <a href="https://www.napier.ac.uk/people/kenny-mitchell"> Kenny Mitchell</a>,
        <a href="http://www.yu-jingyi.com/cv/"> Jingyi Yu</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/1903.10873">Paper</a>
        <a class="btn btn-primary" href="https://github.com/apchenstu/Facial_Details_Synthesis/blob/master/src/imgs/Supplemental_Material.pdf">Supplemental Material</a>
        <a class="btn btn-primary" href="https://github.com/apchenstu/Facial_Details_Synthesis">Code</a>
        <a class="btn btn-primary" href="https://zhuanlan.zhihu.com/p/79242962">Blog</a>
        <a class="btn btn-primary" href="https://drive.google.com/file/d/1VUtS78tWV-jL086c49FID08cHpeoW6mS/view?usp=sharing">Slides</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/HeUyvRtSvfc" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
        <hr>
        <p>
            We present a single-image 3D face synthesis technique that can handle challenging facial expressions while recovering fine geometric details. Our technique employs expression analysis for proxy face geometry generation and combines supervised and unsupervised learning for facial detail synthesis. On proxy generation, we conduct emotion prediction to determine a new expression-informed proxy. On detail synthesis, we present a Deep Facial Detail Net (DFDN) based on Conditional Generative Adversarial Net (CGAN) that employs both geometry and appearance loss functions. For geometry, we capture 366 high-quality 3D scans from 122 different subjects under 3 facial expressions. For appearance, we use additional 163K in-the-wild face images and apply image-based rendering to accommodate lighting variations. Comprehensive experiments demonstrate that our framework can produce high-quality 3D faces with realistic details under challenging facial expressions. 
        </p>
    </div>

    <div class="section">
        <h2>Overview</h2>
        <hr>
        <p>
            From left to right: input face image; proxy 3D face, texture and displacement map produced by our framework; detailed face
            geometry with estimated displacement map applied on the proxy 3D face; and re-rendered facial image.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/teaser.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div> 
    </div>

    <div class="section">
        <h2>Pipeline</h2>
        <hr>
        <p>
            Our processing pipeline.  Top:  training stage for (a) emotion-driven proxy generation and (b) facial detail synthesis.  Bottom:testing stage for an input image.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.png" style="width:100%; margin-right:-10px; margin-top:-10px;">
        </div> 
    </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/1903.10873"
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{chen2019photo,
              title={Photo-Realistic Facial Details Synthesis from Single Image},
              author={Chen, Anpei and Chen, Zhang and Zhang, Guli and Mitchell, Kenny and Yu, Jingyi},
              booktitle={Proceedings of the IEEE International Conference on Computer Vision},
              pages={9429--9439},
              year={2019}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="https://apchenstu.github.io/">Anpei Chen</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>

<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="A novel approach that achieves photo-realistic rendering, fast reconstruction, and compact modeling.">
    <meta name="author" content="Anpei Chen*,
                                Zexiang Xu*,
                                Andreas Geiger,
                                Jingyi Yu,
                                Hao Su">

    <title>Efficient View Path Planning for Autonomous Implicit Reconstruction</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <!-- <h1 class="nerf_title_v2">TensoRF</h1> -->

    <h1 class="nerf_subheader_v2">Efficient View Path Planning for Autonomous Implicit Reconstruction</h1>
        <!-- <h3 class="nerf_subheader_v2">Arxiv 2022 (Under Review)</h3> -->
    <hr>
    <p class="authors">
        <a href="https://small-zeng.github.io/"> Jing Zeng</a>,
        <a href=""> Yanxu Li</a>,
        <a href="https://kingteeloki-ran.github.io/"> Yunlong Ran</a>,
        <a href="https://person.zju.edu.cn/shuoli"> Shuo Li</a>,
        <a href="https://scholar.google.com/citations?user=4RObDv0AAAAJ&hl=zh-CN&oi=sra"> Fei Gao</a>,
        <a href="https://person.zju.edu.cn/shuoli"> Lincheng Li</a>,
        <a href="https://person.zju.edu.cn/shibohe"> Shibo He</a>,
        <a href="https://person.zju.edu.cn/en/jmchen"> Jiming chen</a>,
        <a href="https://person.zju.edu.cn/en/yeqi"> Qi Ye</a>

    </p>

    <!-- <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Equal Contribution</div> -->

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/pdf/2209.13159.pdf">Paper</a>
        <a class="btn btn-primary" href="">Code</a>
        <!-- <a class="btn btn-primary" href="review.pdf">Review</a>
        <a class="btn btn-primary" href="rebuttal.pdf">Rebuttal</a>
        <a class="btn btn-primary" href="meta-review.pdf">Meta-review</a> -->
    </div>
</div>



<div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Overview Video</h2>
<!--    <div class="section">-->
        <div class="vcontainer">
            <iframe class='video' src="../imgs/EVPP.mp4" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>

    </div>

<!--<hr>-->
    </br></br>
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                Implicit neural representations have shown
                promising potential for the 3D scene reconstruction. Recent
                work applies it to autonomous 3D reconstruction by learning
                information gain for view path planning. Effective as it is, the
                computation of the information gain is expensive, and compared
                with that using volumetric representations, collision checking
                using the implicit representation for a 3D point is much slower.
                In the paper, we propose to 1) leverage a neural network as an
                implicit function approximator for the information gain field
                and 2) combine the implicit fine-grained representation with
                coarse volumetric representations to improve efficiency. Further
                with the improved efficiency, we propose a novel informative
                path planning based on a graph-based planner. Our method
                demonstrates significant improvements in the reconstruction
                quality and planning efficiency compared with autonomous
                reconstructions with implicit and explicit representations. We
                deploy the method on a real UAV and the results show that our
                method can plan informative views and reconstruct a scene with
                high quality.
            </p>
            <div class="columns-5 w-row">
                <img src="img/igf_voxel.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>


            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <!-- <p class="paragraph-3 nerf_text">
                We factorize radiance fields into compact components for scene modeling.
                To doso, we apply both the classic CP decomposition and a new vector-matrix (VM) decomposition; both are illustrated in following figure:
            </p>
            <div class="columns-5 w-row">
                <img src="img/tensor_factorizationpng.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
            <p class="paragraph-3 nerf_text">
                Left: CP decomposition, which factorizes atensor as a sum of vector outer products.
                Right: our vector-matrix decomposition, which factorizes a tensor as a sum of vector-matrix outer products.
                Please refer to our paper for more decomposition derails.
            </p>


            <div class="columns-5 w-row">
                <img src="img/pipeline.png" style="width:95%; margin-right:0px; margin-top:10px;">
            </div>

            <p class="paragraph-3 nerf_text">
                We now present our TensoRF representation and reconstruction. For each shading location <b>x</b> = (x,y,z), we use linearly/bilinearly sampled values from the vector (<b>v</b>)/matrix (<b>M</b>) factors to compute the corresponding trilinearly interpolated values of the tensor components.
                The density component values (A<sub>σ</sub>(x)) are summed to get the volume density directly (σ).
                The appearance values (A<sub>c</sub>(x)) are concatenated into a vector (⊕[A<sub>c</sub><sup style="margin-left:-8px">m</sup>(x)]<sub>m</sub>) that is then multiplied by an appearance matrix (<b>B</b>) and sent to the decoding function S for RGB color (c) regression.
                The decoding function S can be a Spherical Harmonic (SH) function or a fully-connected network (FCN).
            </p> -->
        </div>
    </div>



    </br></br>
    <div class="section">
        <s2>Performance</s2>
        <hr>
        <!-- <h2 class="grey-heading_nerf">
                Super Fast Convergence
        </h2> -->
       


        </br></br></br>
        <s2> Related works </s2>
        <hr>
        <!-- <div class="section"> -->
        <p class="paragraph-3 nerf_text">
        Some related works also focus on autonomous implicit reconstruction:</p>
        <!-- <div class="container"> -->
         <ul>
            <li style="text-align: left">NeurAR: Neural Uncertainty for Autonomous 3D Reconstruction<a href="https://kingteeloki-ran.github.io/NeurAR/"> NeurAR </a> </li>
            <!-- <li style="text-align: left">Sparse grid — Plenoxels: Radiance Fields without Neural Networks. <a href="https://alexyu.net/plenoxels/"> Plenoxels </a> </li>
            <li style="text-align: left">Hash — Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. <a href="https://nvlabs.github.io/instant-ngp/"> iNGP </a> </li> -->
         </ul>

        



        <!-- </br></br></br>
        <s2> Acknowledgements </s2>
        <hr>
        <p class="paragraph-3 nerf_text">
            We would like to thank  Yannick Hold-Geoffroy for his useful suggestion in video animation,
            Qiangeng Xu for providing some baseline results,
            and, Katja Schwarz and Michael Niemeyer for providing helpful video materials.
            This project was supported by NSF grant IIS-1764078 and gift money from VIVO.
        </p> -->


        </br></br>
        <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
            @INPROCEEDINGS{Arxiv2022,
              author = {Jing Zeng, Yanxu Li, Yunlong Ran, Shuo Li, Fei Gao, Lincheng Li, 
                        Shibo He, Jiming chen, Qi Ye},
              title = {Efficient View Path Planning for Autonomous Implicit Reconstruction},
              booktitle = {},
              year = {2022}
            }
        </div>
<!--    </div>-->
    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from NeRF.
            Send feedback and questions to <a href="https://apchenstu.github.io/">Anpei Chen</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>

<!DOCTYPE html>

<html lang="en">
<head>

    <title>Efficient View Path Planning for Autonomous Implicit Reconstruction</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">

    <style>
        .w-container {
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 宽高比 */
            position: relative;
        }

        .w-container video {
            position: absolute;
            width: 100%;
            height: 100%;
        }
    </style>


</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <!-- <h1 class="nerf_title_v2">TensoRF</h1> -->

    <h1 class="nerf_subheader_v2">Efficient View Path Planning for Autonomous Implicit Reconstruction</h1>
        <!-- <h3 class="nerf_subheader_v2">Arxiv 2022 (Under Review)</h3> -->
    <hr>
    <p class="authors">
        <a href="https://small-zeng.github.io/"> Jing Zeng</a>,
        <a href=""> Yanxu Li</a>,
        <a href="https://kingteeloki-ran.github.io/"> Yunlong Ran</a>,
        <a href="https://person.zju.edu.cn/shuoli"> Shuo Li</a>,
        <a href="https://scholar.google.com/citations?user=4RObDv0AAAAJ&hl=zh-CN&oi=sra"> Fei Gao</a>,
        <a href=""> Lincheng Li</a>,
        <a href="https://person.zju.edu.cn/shibohe"> Shibo He</a>,
        <a href="https://person.zju.edu.cn/en/jmchen"> Jiming chen</a>,
        <a href="https://person.zju.edu.cn/en/yeqi"> Qi Ye</a>

    </p>

    <!-- <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Equal Contribution</div> -->

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://ieeexplore.ieee.org/abstract/document/10160793">Paper</a>
        <a class="btn btn-primary" href="https://github.com/small-zeng/EVPP">Code</a>
    </div>
</div>



<!-- <div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Overview Video</h2>
        <video controls="controls">
            <source src="../imgs/EVPP.mp4" type="video/mp4">
        </video>
    </div>
</div> -->

<!-- <div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Overview Video</h2>
        <iframe width="560" height="315" src="https://www.youtube.com/watch?v=pZfZmH5qxMk" frameborder="0" allowfullscreen></iframe>
    </div>
</div> -->

<div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Overview Video</h2>
        <video controls="controls">
            <source src="https://www.youtube.com/watch?v=pZfZmH5qxMk" type="video/mp4">
        </video>
    </div>
</div>






<br><br><br><br><br><br>
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                Implicit neural representations have shown
                promising potential for the 3D scene reconstruction. Recent
                work applies it to autonomous 3D reconstruction by learning
                information gain for view path planning. Effective as it is, the
                computation of the information gain is expensive, and compared
                with that using volumetric representations, collision checking
                using the implicit representation for a 3D point is much slower.
                In the paper, we propose to 1) leverage a neural network as an
                implicit function approximator for the information gain field
                and 2) combine the implicit fine-grained representation with
                coarse volumetric representations to improve efficiency. Further
                with the improved efficiency, we propose a novel informative
                path planning based on a graph-based planner. Our method
                demonstrates significant improvements in the reconstruction
                quality and planning efficiency compared with autonomous
                reconstructions with implicit and explicit representations. We
                deploy the method on a real UAV and the results show that our
                method can plan informative views and reconstruct a scene with
                high quality.
            </p>
            <div class="columns-5 w-row">
                <img src="img/igf_voxel.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
            <br><br><br><br><br><br>


           
        </div>
    </div>
 
     
    <br><br><br><br><br><br>
    <div class="section">
        <h2 class="grey-heading_nerf">
            Method
        </h2>
        <p class="paragraph-3 nerf_text">
            The problem considered is to generate a trajectory for a robot that yields high-quality 3D models of a bounded 
            target scene and fulfills robot constraints like time and path length. However, finding
            the best sequence of viewpoints is time-consuming and prohibitively expensive. We adopt the
            greedy strategy and trade it as a Next-Best-View (NBV)
            problem. At each step of the reconstruction, the information
            gain of viewpoints within a certain range of the current
            position of a robot are evaluated based on the partial reconstruction
            scene. An informative path considering both the
            information gain and the path length is then planned and
            executed. Images captured along the view path are selected
            and are fed into the 3D reconstruction of next step. The
            process is repeated until some critera are met.
        </p>

        <p class="paragraph-3 nerf_text">
            Under the greedy strategy, our pipeline consists of three
            components: a Mobile Robot module, a 3D Reconstruction
            module and a View Path Planning module and shown as below: 

        </p>
        <br><br><br><br><br><br>
        <div class="columns-5 w-row">
            <img src="img/overview.png" style="width:95%; margin-right:0px; margin-top:0px;">
        </div>
        <br><br><br><br><br><br>
        
        <p class="paragraph-3 nerf_text">
            The Mobile Robot module takes the images at given viewpoints
            and the robot locates itself by a motion capture system.
            During the simulation, Unity Engine renders images at
            given viewpoints. The 3D Reconstruction module reconstructs
            a scene by combining an implicit neural representation
            (e.g. NeRF) and a volumetric representation (A
            coarse TSDF). The implicit neural representation provides
            high quality 3D models with fine-grained details and also
            neural uncertainty as the information gain for the view path
            planning. The coarse TSDF filter viewpoints and establishes
            an occupancy grid map for efficient distance and occupancy
            query, and viewpoint filtering. The View Path Planning
            module first leverages volumetric representations for efficient
            viewpoint selection, approximates the information gain field
            by a MLP and plans an informative view path based the
            A* algorithm.
        </p>

        <s2>Performance</s2>
        <hr>
        <h2 class="grey-heading_nerf">
            Comparison of the reconstruction scenes
        </h2>
        </br></br>
        <div class="columns-5 w-row">
            <img src="img/view_compare.png" style="width:95%; margin-right:0px; margin-top:0px;">
        </div>
        <h2 class="grey-heading_nerf">
            Trajectories and the reconstruction results 
        </h2>
        </br></br>
        <div class="columns-5 w-row">
            <img src="img/trajectory_compare.png" style="width:60%; margin-right:0px; margin-top:0px;">
        </div>
        <p class="paragraph-3 nerf_text">
            The results are seen from top view in childroom scene. 
        </p>
        <h2 class="grey-heading_nerf">
            The experiment on a real scene 
        </h2>
        </br></br>
        <div class="columns-5 w-row">
            <img src="img/real_sence_result.png" style="width:95%; margin-right:0px; margin-top:0px;">
        </div>
        


        </br></br></br>
        <s2> Related works </s2>
        <hr>
        <!-- <div class="section"> -->
        <p class="paragraph-3 nerf_text">
        Some related works also focus on autonomous implicit reconstruction:</p>
        <!-- <div class="container"> -->
         <ul>
            <li style="text-align: left">NeurAR: Neural Uncertainty for Autonomous 3D Reconstruction<a href="https://kingteeloki-ran.github.io/NeurAR/"> NeurAR </a> </li>
            <!-- <li style="text-align: left">Sparse grid — Plenoxels: Radiance Fields without Neural Networks. <a href="https://alexyu.net/plenoxels/"> Plenoxels </a> </li>
            <li style="text-align: left">Hash — Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. <a href="https://nvlabs.github.io/instant-ngp/"> iNGP </a> </li> -->
         </ul>

        



        </br></br>
        <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{zeng2023efficient,
                title={Efficient view path planning for autonomous implicit reconstruction},
                author={Zeng, Jing and Li, Yanxu and Ran, Yunlong and Li, Shuo and Gao, Fei and Li, Lincheng and He, Shibo and Chen, Jiming and Ye, Qi},
                booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
                pages={4063--4069},
                year={2023},
                organization={IEEE}
              }
        </div>
<!--    </div>-->
    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from NeRF.
            Send feedback and questions to <a href="https://small-zeng.github.io/">Jing Zeng</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>


</body>
</html>
